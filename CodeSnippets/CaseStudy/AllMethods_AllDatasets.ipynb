{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "\n",
    "from scipy.spatial.distance import euclidean, hamming\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gurobi_NN(X, y, k):\n",
    "    np.random.seed(96)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    print(X_train.shape)\n",
    "    N, I = X_train.shape\n",
    "    M, I = X_test.shape\n",
    "    p = gp.Model(\"NN\")\n",
    "\n",
    "    # Add variables\n",
    "    alpha = p.addVars(I, lb=0, name=\"alpha\", vtype=GRB.CONTINUOUS)\n",
    "    e = p.addVars(M, name=\"e\", vtype=GRB.CONTINUOUS)\n",
    "    y_pred = p.addVars(M, name=\"y_pred\", vtype=GRB.CONTINUOUS, lb=-GRB.INFINITY)\n",
    "    dist = p.addVars(N, M, name=\"dist\", vtype=GRB.CONTINUOUS)\n",
    "    sqdist = p.addVars(N, M, name=\"sqdist\", vtype=GRB.CONTINUOUS)\n",
    "\n",
    "    # Add constraints\n",
    "    for m in range(M):\n",
    "        p.addConstr(y_pred[m] == 1/N * gp.quicksum(float(y_train[n]) * \n",
    "                                                   (1 + \n",
    "                                                    1/N * 1/k * gp.quicksum(dist[n1,m] for n1 in range(N))) -\n",
    "                                                    1/k * dist[n,m]                                   \n",
    "                                                    for n in range(N)), name=\"y_pred\"+str(m))\n",
    "        p.addConstr(e[m] >= float(y_test[m]) - y_pred[m], name=\"e+\"+str(m))\n",
    "        p.addConstr(e[m] >= y_pred[m] - float(y_test[m]), name=\"e-\"+str(m))\n",
    "        for n in range(N):\n",
    "            p.addConstr(dist[n,m] == gp.quicksum(alpha[i] * (X_train[n][i]-X_test[m][i])**2  for i in range(I)))\n",
    "            p.addConstr(sqdist[n,m]*sqdist[n,m] == dist[n,m])\n",
    "    \n",
    "    for i in range(I):\n",
    "        p.addConstr(alpha[i] >= 1e-5, name=\"alpha+\"+str(i))\n",
    "        p.addConstr(alpha[i] <= 1/2, name=\"alpha-\"+str(i))\n",
    "\n",
    "    p.setObjective(1/M * gp.quicksum(e[m] for m in range(M)), GRB.MINIMIZE)\n",
    "\n",
    "    # Don't print output\n",
    "    p.setParam('OutputFlag', 0)\n",
    "    p.optimize()\n",
    "\n",
    "    if p.status == GRB.Status.OPTIMAL:\n",
    "        return [np.abs(alpha[i].x) for i in range(I)], X_train, y_train\n",
    "    else:\n",
    "        p.computeIIS()\n",
    "        p.write(\"model1.ilp\")\n",
    "        return [1/I for i in range(I)], X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with gurobi\n",
    "def proposed_regression(X_train, y_train, X_test, k):\n",
    "    alpha = gurobi_NN(X_train, y_train, k)[0]\n",
    "    # Predict using alphas as weights for the distance + consider weighted (with distance) average of the k nearest neighbors\n",
    "    y_pred = []\n",
    "    for m in range(X_test.shape[0]):\n",
    "        dist = []\n",
    "        for n in range(X_train.shape[0]):\n",
    "            dist.append((np.sum(np.abs(alpha * (X_test[m] - X_train[n])**(2)))))\n",
    "        idx = np.argsort(dist)[:k]\n",
    "        y_pred.append(np.sum([y_train[i]/(1+dist[i]) for i in idx])/np.sum([1/(1+dist[i]) for i in idx]))\n",
    "        if np.isnan(y_pred[-1]):\n",
    "            y_pred[-1] = np.mean(y_train)\n",
    "            print('nan')\n",
    "    return y_pred\n",
    "\n",
    "def proposed_classification(X_train, y_train, X_test, k):\n",
    "    alpha = gurobi_NN(X_train, y_train, k)[0]\n",
    "    # Predict using alphas as weights for the distance + consider weighted (with distance) average of the k nearest neighbors\n",
    "    y_pred = []\n",
    "    for m in range(X_test.shape[0]):\n",
    "        dist = []\n",
    "        for n in range(X_train.shape[0]):\n",
    "            dist.append(np.sum(np.abs(alpha * (X_test[m] - X_train[n])**2)))\n",
    "        idx = np.argsort(dist)[:k]\n",
    "        weights = [1 / (1 + dist[i]) for i in idx]\n",
    "        y_pred.append(max(set(y_train[idx]), key=lambda x: sum(w for i, w in zip(idx, weights) if y_train[i] == x)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_regression(X_train, y_train, X_test, k):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def traditional_classification(X_train, y_train, X_test, k):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_regression(X_train, y_train, X_test, k):\n",
    "    I = X_train.shape[1]\n",
    "    weights = [1/I for _ in range(I)]\n",
    "    y_predict = []\n",
    "\n",
    "    for x in X_test:\n",
    "        similarities = []\n",
    "        for i, x_train in enumerate(X_train):\n",
    "            sim = 0\n",
    "            for j in range(I):\n",
    "                if x[j] == x_train[j]:\n",
    "                    sim += weights[j]\n",
    "            similarities.append((sim, y_train[i]))\n",
    "\n",
    "        # Sort by similarity in descending order and select top k\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        similarities = similarities[:k]\n",
    "\n",
    "        # Calculate the predicted value as the average of the top k neighbors' values\n",
    "        y_pred = sum(y for _, y in similarities) / k\n",
    "        y_predict.append(y_pred)\n",
    "\n",
    "    return y_predict\n",
    "\n",
    "def overlap_classification(X_train, y_train, X_test, k):\n",
    "    I = X_train.shape[1]\n",
    "    N = X_test.shape[0]\n",
    "    weights = [1/I for i in range(I)]\n",
    "    y_predict = []\n",
    "    for x in X_test:\n",
    "        similitudes = []\n",
    "        for x_train in X_train:\n",
    "            sim = 0\n",
    "            for i in range(I):\n",
    "                if x[i] != x_train[i]:\n",
    "                    s = 0\n",
    "                else:\n",
    "                    s = 1\n",
    "                sim += s * weights[i]\n",
    "            similitudes.append([sim, y_train[i]])\n",
    "        similitudes.sort(key=lambda x: x[0])\n",
    "        similitudes = similitudes[:k]\n",
    "        labels = [x[1] for x in similitudes]\n",
    "        y_predict.append(max(set(labels), key=labels.count))\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eskin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eskin_classification(X_train, y_train, X_test, k):\n",
    "    I = X_train.shape[1]\n",
    "    weights = [1/I for _ in range(I)]\n",
    "    cardinality_features = [len(set(X_train[:, i])) for i in range(I)]\n",
    "    y_predict = []\n",
    "\n",
    "    for x in X_test:\n",
    "        similarities = []\n",
    "        for i, x_train in enumerate(X_train):\n",
    "            sim = 0\n",
    "            for j in range(I):\n",
    "                if x[j] == x_train[j]:\n",
    "                    sim += weights[j]\n",
    "                else:\n",
    "                    n_k = cardinality_features[j]\n",
    "                    sim += n_k**2 / (n_k**2 + 2) * weights[j]\n",
    "            similarities.append((sim, y_train[i]))\n",
    "\n",
    "        # Sort by similarity in descending order and select top k\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        similarities = similarities[:k]\n",
    "\n",
    "        # Predict the most common class among the top k neighbors\n",
    "        y_pred = max(set(y for _, y in similarities), key=[y for _, y in similarities].count)\n",
    "        y_predict.append(y_pred)\n",
    "\n",
    "    return y_predict\n",
    "\n",
    "def Eskin_regression(X_train, y_train, X_test, k):\n",
    "    I = X_train.shape[1]\n",
    "    weights = [1/I for _ in range(I)]\n",
    "    cardinality_features = [len(set(X_train[:, i])) for i in range(I)]\n",
    "    y_predict = []\n",
    "\n",
    "    for x in X_test:\n",
    "        similarities = []\n",
    "        for i, x_train in enumerate(X_train):\n",
    "            sim = 0\n",
    "            for j in range(I):\n",
    "                if x[j] == x_train[j]:\n",
    "                    sim += weights[j]\n",
    "                else:\n",
    "                    n_k = cardinality_features[j]\n",
    "                    sim += n_k**2 / (n_k**2 + 2) * weights[j]\n",
    "            similarities.append((sim, y_train[i]))\n",
    "\n",
    "        # Sort by similarity in descending order and select top k\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        similarities = similarities[:k]\n",
    "\n",
    "        # Calculate the predicted value as the average of the top k neighbors' values\n",
    "        y_pred = sum(y for _, y in similarities) / k\n",
    "        y_predict.append(y_pred)\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def of_classification(X_train, y_train, X_test, k):\n",
    "    I = X_train.shape[1]\n",
    "    weights = [1/I for _ in range(I)]\n",
    "    frequency_features = [{} for _ in range(I)]\n",
    "\n",
    "    # Calculate the frequency of each feature value\n",
    "    for i in range(I):\n",
    "        for x in X_train[:, i]:\n",
    "            if x in frequency_features[i]:\n",
    "                frequency_features[i][x] += 1\n",
    "            else:\n",
    "                frequency_features[i][x] = 1\n",
    "\n",
    "    y_predict = []\n",
    "\n",
    "    for x in X_test:\n",
    "        similarities = []\n",
    "        for i, x_train in enumerate(X_train):\n",
    "            sim = 0\n",
    "            for j in range(I):\n",
    "                if x[j] == x_train[j]:\n",
    "                    sim += weights[j]\n",
    "                else:\n",
    "                    freq_x = frequency_features[j].get(x[j], 1)\n",
    "                    freq_x_train = frequency_features[j].get(x_train[j], 1)\n",
    "                    sim += (1 / (1 + np.log(freq_x) * np.log(freq_x_train))) * weights[j]\n",
    "            similarities.append((sim, y_train[i]))\n",
    "\n",
    "        # Sort by similarity in descending order and select top k\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        similarities = similarities[:k]\n",
    "\n",
    "        # Predict the most common class among the top k neighbors\n",
    "        y_pred = max(set(y for _, y in similarities), key=[y for _, y in similarities].count)\n",
    "        y_predict.append(y_pred)\n",
    "\n",
    "    return y_predict\n",
    "\n",
    "def of_regression(X_train, y_train, X_test, k):\n",
    "    I = X_train.shape[1]\n",
    "    weights = [1/I for _ in range(I)]\n",
    "    frequency_features = [{} for _ in range(I)]\n",
    "\n",
    "    # Calculate the frequency of each feature value\n",
    "    for i in range(I):\n",
    "        for x in X_train[:, i]:\n",
    "            if x in frequency_features[i]:\n",
    "                frequency_features[i][x] += 1\n",
    "            else:\n",
    "                frequency_features[i][x] = 1\n",
    "\n",
    "    y_predict = []\n",
    "\n",
    "    for x in X_test:\n",
    "        similarities = []\n",
    "        for i, x_train in enumerate(X_train):\n",
    "            sim = 0\n",
    "            for j in range(I):\n",
    "                if x[j] == x_train[j]:\n",
    "                    sim += weights[j]\n",
    "                else:\n",
    "                    freq_x = frequency_features[j].get(x[j], 1)\n",
    "                    freq_x_train = frequency_features[j].get(x_train[j], 1)\n",
    "                    sim += (1 / (1 + np.log(freq_x) * np.log(freq_x_train))) * weights[j]\n",
    "            similarities.append((sim, y_train[i]))\n",
    "\n",
    "        # Sort by similarity in descending order and select top k\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        similarities = similarities[:k]\n",
    "\n",
    "        # Calculate the predicted value as the average of the top k neighbors' values\n",
    "        y_pred = sum(y for _, y in similarities) / k\n",
    "        y_predict.append(y_pred)\n",
    "\n",
    "    return y_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gwknn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizedWeightedKNN:\n",
    "    def __init__(self, k=3, beta=8, learning_rate=0.1, max_iter=100):\n",
    "        self.k = k\n",
    "        self.beta = beta\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.weights = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.weights = np.ones((n_samples,))\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            for i in range(n_samples):\n",
    "                self._update_weights(X_train, y_train, i)\n",
    "\n",
    "    def _update_weights(self, X, y, idx):\n",
    "        distances = self._compute_distances(X, idx)\n",
    "        nearest_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "        enemy_loss = 0\n",
    "        friend_loss = 0\n",
    "\n",
    "        for neighbor_idx in nearest_indices:\n",
    "            if y[neighbor_idx] != y[idx]:\n",
    "                enemy_loss += 1\n",
    "            else:\n",
    "                friend_loss += 1\n",
    "\n",
    "        total_loss = (enemy_loss + friend_loss) / 2\n",
    "        gradient = self._compute_gradient(X, y, idx, nearest_indices, total_loss)\n",
    "        self.weights[idx] -= self.learning_rate * gradient\n",
    "\n",
    "    def _compute_distances(self, X, idx):\n",
    "        distances = np.linalg.norm(X - X[idx], axis=1) / self.weights\n",
    "        return distances\n",
    "\n",
    "    def _compute_gradient(self, X, y, idx, nearest_indices, total_loss):\n",
    "        gradient = 0\n",
    "        for neighbor_idx in nearest_indices:\n",
    "            if y[neighbor_idx] != y[idx]:\n",
    "                gradient += self._sigmoid_derivative(total_loss) * (1 / self.weights[idx])\n",
    "            else:\n",
    "                gradient -= self._sigmoid_derivative(total_loss) * (1 / self.weights[idx])\n",
    "        return gradient\n",
    "\n",
    "    def _sigmoid_derivative(self, z):\n",
    "        sigmoid = 1 / (1 + np.exp(-self.beta * (1 - z)))\n",
    "        return self.beta * sigmoid * (1 - sigmoid)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for x in X_test:\n",
    "            distances = np.linalg.norm(self.X_train - x, axis=1) / self.weights\n",
    "            nearest_indices = np.argsort(distances)[:self.k]\n",
    "            nearest_labels = self.y_train[nearest_indices]\n",
    "            prediction = np.round(np.mean(nearest_labels)).astype(int)\n",
    "            predictions.append(prediction)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_regression(self, X_test):\n",
    "        predictions = []\n",
    "        for x in X_test:\n",
    "            distances = np.linalg.norm(self.X_train - x, axis=1) / self.weights\n",
    "            nearest_indices = np.argsort(distances)[:self.k]\n",
    "            nearest_values = self.y_train[nearest_indices]\n",
    "            prediction = np.average(nearest_values, weights=1/distances[nearest_indices])\n",
    "            predictions.append(prediction)\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_models_classification(X_train, y_train, X_test, y_test, k):\n",
    "    models = {\n",
    "        \"Traditional KNN\": traditional_classification,\n",
    "        \"Overlap KNN\": overlap_classification,\n",
    "        \"Eskin KNN\": Eskin_classification,\n",
    "        \"OF KNN\": of_classification,\n",
    "        \"Proposed Model\": proposed_classification,\n",
    "        \"Generalized Weighted KNN\": GeneralizedWeightedKNN(k=k)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        if name == \"Generalized Weighted KNN\":\n",
    "            model_instance = model\n",
    "            model_instance.fit(X_train, y_train)\n",
    "            y_pred = model_instance.predict(X_test)\n",
    "        else:\n",
    "            y_pred = model(X_train, y_train, X_test, k)\n",
    "        results[name] = y_pred\n",
    "\n",
    "    # Calculate accuracy for each model\n",
    "    accuracies = {}\n",
    "    for name, y_pred in results.items():\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        accuracies[name] = accuracy\n",
    "        print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "    return accuracies\n",
    "\n",
    "def all_models_regression(X_train, y_train, X_test, y_test, k):\n",
    "    models = {\n",
    "        \"Traditional KNN\": traditional_regression,\n",
    "        \"Overlap KNN\": overlap_regression,\n",
    "        \"Eskin KNN\": Eskin_regression,\n",
    "        \"OF KNN\": of_regression,\n",
    "        \"Proposed Model\": proposed_regression,\n",
    "        \"Generalized Weighted KNN\": GeneralizedWeightedKNN(k=k)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        if name == \"Generalized Weighted KNN\":\n",
    "            model_instance = model\n",
    "            model_instance.fit(X_train, y_train)\n",
    "            y_pred = model_instance.predict_regression(X_test)\n",
    "        else:\n",
    "            y_pred = model(X_train, y_train, X_test, k)\n",
    "        results[name] = y_pred\n",
    "\n",
    "    # Calculate RMSE for each model and R^2\n",
    "    rmse_scores = {}\n",
    "    r2_scores = {}\n",
    "    for name, y_pred in results.items():\n",
    "        # if y_pred contains NaN, replace with mean of y_test\n",
    "        if np.isnan(y_pred).any():\n",
    "            y_pred = np.nan_to_num(y_pred, nan=np.mean(y_test))\n",
    "        r2 = 1 - (np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))\n",
    "        print(f\"{name} R^2: {r2:.4f}\")\n",
    "        r2_scores[name] = r2\n",
    "    return r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_k(X_train, y_train, X_test, y_test, class_regr=1):\n",
    "    ks = [k for k in range(2, 13)]\n",
    "    if class_regr == 1:\n",
    "        accuracies = {}\n",
    "        for k in ks:\n",
    "            accuracies[k] = all_models_classification(X_train, y_train, X_test, y_test, k)\n",
    "            print('accuracies for k =', k)\n",
    "            print(accuracies[k])\n",
    "        return accuracies\n",
    "    else:\n",
    "        r2_scores = {}\n",
    "        for k in ks:\n",
    "            r2_scores[k] = all_models_regression(X_train, y_train, X_test, y_test, k)\n",
    "        return r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_splits(X, y, class_regr=1):\n",
    "    accuracies = {}\n",
    "    r2_scores = {}\n",
    "    for i in range(10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i*100+(10-i))\n",
    "        # Then use multiple_k function, so that we have all the k values tested\n",
    "        if class_regr == 1:\n",
    "            accuracy = multiple_k(X_train, y_train, X_test, y_test, class_regr)\n",
    "            print('accuracies for split', i)\n",
    "            print(accuracy)\n",
    "            accuracies[i] = accuracy\n",
    "        else:\n",
    "            r2 = multiple_k(X_train, y_train, X_test, y_test, class_regr)\n",
    "            r2_scores[i] = r2\n",
    "    if class_regr == 1:\n",
    "        return accuracies\n",
    "    else:\n",
    "        return r2_scores\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results in a csv file with the name of the dataset\n",
    "def save_results_to_csv(results, dataset_name, class_regr=1):\n",
    "    if class_regr == 1:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(f\"{dataset_name}_classification_results.csv\", index=False)\n",
    "    else:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(f\"{dataset_name}_regression_results.csv\", index=False)\n",
    "    print(f\"Results saved to {dataset_name}_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Houses votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from house-votes-84.csv\n",
    "data_hv84 = pd.read_csv('house-votes-84.csv')\n",
    "data_hv84 = data_hv84.replace('y', 1)\n",
    "data_hv84 = data_hv84.replace('n', 0)\n",
    "data_hv84 = data_hv84.replace('?', 0.5)\n",
    "data_hv84 = data_hv84.replace('democrat', 1)\n",
    "data_hv84 = data_hv84.replace('republican', 0)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_hv84 = data_hv84.drop('party', axis=1).to_numpy()\n",
    "y_hv84 = data_hv84['party'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = multiple_splits(X_hv84, y_hv84, class_regr=1)\n",
    "save_results_to_csv(accuracies, 'house-votes-84', class_regr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pima indians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from pima-indians-diabetes.csv\n",
    "data_pima = pd.read_csv('pima-indians-diabetes.csv')\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_pima = data_pima.drop('Outcome', axis=1).to_numpy()\n",
    "y_pima = data_pima['Outcome'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = multiple_splits(X_pima, y_pima, class_regr=1)\n",
    "save_results_to_csv(accuracies, 'pima_indians', class_regr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ionosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from ionosphere/ionosphere.data\n",
    "\n",
    "data_ionosphere = pd.read_csv('ionosphere/ionosphere.data', header=None)\n",
    "\n",
    "data_ionosphere = data_ionosphere.replace('g', 1)\n",
    "data_ionosphere = data_ionosphere.replace('b', 0)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_ionosphere = data_ionosphere.drop(34, axis=1).to_numpy()\n",
    "y_ionosphere = data_ionosphere[34].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = multiple_splits(X_ionosphere, y_ionosphere, class_regr=1)\n",
    "save_results_to_csv(accuracies, 'ionosphere', class_regr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from insurance.csv\n",
    "data_insurance = pd.read_csv('insurance.csv')\n",
    "data_insurance = data_insurance.replace('female', 1)\n",
    "data_insurance = data_insurance.replace('male', 0)\n",
    "data_insurance = data_insurance.replace('yes', 1)\n",
    "data_insurance = data_insurance.replace('no', 0)\n",
    "data_insurance = data_insurance.replace('southwest', 0)\n",
    "data_insurance = data_insurance.replace('southeast', 1)\n",
    "data_insurance = data_insurance.replace('northwest', 2)\n",
    "data_insurance = data_insurance.replace('northeast', 3)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_insurance = data_insurance.drop('charges', axis=1).to_numpy()\n",
    "y_insurance = data_insurance['charges'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = multiple_splits(X_insurance, y_insurance, class_regr=0)\n",
    "save_results_to_csv(r2, 'insurance', class_regr=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steel Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Steel_industry_data.csv')\n",
    "# Transform date column - example : 01/01/2018 00:15\n",
    "data['date'] = pd.to_datetime(data['date'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "# Select only when hour is 00, 08, 16\n",
    "data = data[data['date'].dt.hour.isin([0, 8, 16])]\n",
    "data = data[data['date'].dt.minute == 0]\n",
    "\n",
    "# Encode date column - drop year, month. Day, hour and minute are 3 separate columns\n",
    "data['day'] = data['date'].dt.day\n",
    "data['hour'] = data['date'].dt.hour\n",
    "data['minute'] = data['date'].dt.minute\n",
    "data = data.drop(columns=['date'])\n",
    "\n",
    "# Encode WeekStatus\n",
    "data['WeekStatus'] = data['WeekStatus'].replace({'Weekday': 0, 'Weekend': 1})\n",
    "\n",
    "# Encode Day_of_week\n",
    "data['Day_of_week'] = data['Day_of_week'].replace({'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3,\n",
    "                                                    'Friday': 4, 'Saturday': 5, 'Sunday': 6})\n",
    "\n",
    "# Encode Load_Type using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data['Load_Type'] = label_encoder.fit_transform(data['Load_Type'])\n",
    "\n",
    "# X, y, y is Load_Type\n",
    "X_steel = data.drop(columns=['Load_Type']).to_numpy()\n",
    "y_steel = data['Load_Type'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = multiple_splits(X_steel, y_steel, class_regr=1)\n",
    "save_results_to_csv(accuracies, 'steel', class_regr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Servo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('servo.data')\n",
    " \n",
    "# Encode columns\n",
    "label_encoder = LabelEncoder()\n",
    "data['motor'] = label_encoder.fit_transform(data['motor'])\n",
    "data['screw'] = label_encoder.fit_transform(data['screw'])\n",
    "\n",
    "# Split X and y - y is class\n",
    "X_servo = data.drop(columns=['class'])\n",
    "y_servo = data['class']\n",
    "X_servo = X_servo.to_numpy()\n",
    "y_servo = y_servo.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = multiple_splits(X_servo, y_servo, class_regr=0)\n",
    "save_results_to_csv(accuracies, 'servo', class_regr=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liver Disorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from Liver_disorder.csv\n",
    "data_liver = pd.read_csv('Liver_disorder.csv')\n",
    "\n",
    "data_liver = data_liver.replace('Female', 1)\n",
    "data_liver = data_liver.replace('Male', 0)\n",
    "data_liver = data_liver.fillna(data_liver.mean())\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_liver = data_liver.drop('Selector', axis=1).to_numpy()\n",
    "y_liver = data_liver['Selector'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = multiple_splits(X_liver, y_liver, class_regr=1)\n",
    "save_results_to_csv(accuracies, 'liver', class_regr=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
